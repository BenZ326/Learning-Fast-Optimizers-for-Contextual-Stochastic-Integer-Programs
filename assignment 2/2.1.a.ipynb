{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP767: Reinforcement Learning\n",
    "\n",
    "## Assignment 2\n",
    "\n",
    "- Rahul Patel, Polytechnique Montreal, 1933839\n",
    "- Xiangyi Zhang, Polytechnique Montreal,\n",
    "\n",
    "\n",
    "Use a tabular representation of the state space, and ensure that the starting and end location\n",
    "of the passenger are random. Exploration should be softmax. You will need to run the\n",
    "following protocol. You will do 10 independent runs. Each run consists of 100 segments, in\n",
    "each segment there are 10 episodes of training, followed by 1 episode in which you simply\n",
    "run the optimal policy so far. Pick 3 settings of the temperature parameter and 3 settings of\n",
    "the learning rate. You need to plot:\n",
    "\n",
    "- One u-shaped graph that shows the effect of the parameters on the final training performance (see the book)\n",
    "- One u-shaped graph that shows the effect of the parameters on the final testing performance (see the book)\n",
    "- Learning curves (mean and standard deviation) for what you pick as the best parameter\n",
    "\n",
    "Setting for each algorithm\n",
    "Write a small report that describes your experiment, your choices of parameters, and the\n",
    "conclusions you draw from the graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rahul/anaconda3/envs/ML/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import gym\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "actions = np.arange(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def q_update(Q,temp=0.9,lr=0.01):\n",
    "    rewards = []\n",
    "    state = env.reset()\n",
    "    \n",
    "    for t in itertools.count():\n",
    "        # Softmax exploration\n",
    "        action = np.random.choice(actions, p=softmax(Q[state]))                \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        td_target = reward + temp * np.max(Q[next_state])\n",
    "        td_delta = td_target - Q[state][action]\n",
    "        Q[state][action] += lr * td_delta\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    \n",
    "    return Q, t, np.sum(rewards)\n",
    "\n",
    "def sarsa_update(Q,temp=0.9,lr=0.01):\n",
    "    rewards = []\n",
    "    state = env.reset()\n",
    "    \n",
    "    for t in itertools.count():\n",
    "        action = np.random.choice(actions, p=softmax(Q[state]))                \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        next_action = np.random.choice(Q[next_state], p=softmax(Q[next_state]))                \n",
    "        td_target = reward + temp * Q[next_state][next_action]        \n",
    "        td_delta = td_target - Q[state][action]\n",
    "        Q[state][action] += lr * td_delta\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "            \n",
    "    return Q, t, np.sum(rewards)\n",
    "\n",
    "\n",
    "def expected_sarsa_update(Q,temp=0.9,lr=0.01):\n",
    "    rewards = []\n",
    "    state = env.reset()\n",
    "    \n",
    "    for t in itertools.count():\n",
    "        action = np.random.choice(actions, p=softmax(Q[state]))                \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        naction_probs = softmax(Q[next_state])        \n",
    "        td_target = reward + temp * np.sum(next_action_probs * Q[next_state])        \n",
    "        td_delta = td_target - Q[state][action]\n",
    "        Q[state][action] += lr * td_delta\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "        \n",
    "    return Q, t, np.sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-956baee6904d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q_learning'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q_learning'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q_learning'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q_learning'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "RUNS = 10\n",
    "SEGMENTS = 100\n",
    "EPISODES = 10\n",
    "lr_rate = [0.9, 0.1, 0.01]\n",
    "temperature = [0.9]\n",
    "\n",
    "Q = defaultdict(list)\n",
    "t = defaultdict(list)\n",
    "rewards = defaultdict(list)\n",
    "\n",
    "Q['q_learning'] = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "Q['sarsa'] = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "Q['expected_sarsa'] = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "# for run in range(RUNS):\n",
    "#     for segment in range(SEGMENTS):\n",
    "#         for episode in range(EPISODES):            \n",
    "#             # Train\n",
    "#             if episode != EPISODES-1:                                \n",
    "#             # Test\n",
    "#             else:    \n",
    "\n",
    "\n",
    "for i in range(500):    \n",
    "    Q['q_learning'], timesteps, total_reward = q_update(Q['q_learning'])\n",
    "    t['q_learning'].append(time)\n",
    "    rewards['q_learning'].append(total_reward)\n",
    "    \n",
    "#     Q['sarsa'], t, total_reward = sarsa_update(Q['sarsa'])\n",
    "#     t['q_learning'].append(t)\n",
    "#     rewards['q_learning'].append(total_reward)\n",
    "    \n",
    "#     Q['expected_sarsa'], t, total_reward = expected_sarsa_update(Q['expected_sarsa'])\n",
    "#     t['q_learning'].append(t)\n",
    "#     rewards['q_learning'].append(total_reward)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Q['q_learning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
