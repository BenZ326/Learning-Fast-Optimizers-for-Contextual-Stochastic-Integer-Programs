{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best-armidentiﬁcation: also sometimes called pure exploration,this is a problem formulation in which the learner is allowed a period of experimentation of a fixed length after which it has to act forever according to the policy it identifies as being best.No further learning is allowed after the \"pure exploration\" phase. For this part of the assignment you will have to read the following paper: Best-arm identiﬁcation algorithms for multi-armed bandits in the fixed confidence setting, Kevin Jamieson and Robert Nowak, CISS, 2014: https://people.eecs.berkeley.edu/ kjamieson/resources/bestArmSurvey.pdf The paper describes two different classes of algorithms for this problem. Your task is to: (a) summarize the main results in the paper; (b) Reproduce the results in Figure 1 (c) Perform the same empirical comparison on the bandit problem provided in the Sutton & Barto book (which we discussed in class). Do not forget to average your results over multiple independent runs. (d) discuss in a short paragraph a concrete application in which you think regret optimization would be more useful than best arm identiﬁcation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Summarize the main results in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper presents a review over two main types of algorithms that have optimal sample complexity for pure \"exploration\" problem in the fixed confidence setting. The main contribution of the paper is the qualitative, quantitative overview of the algorithms and developing novel sample complexity bounds.They conducted two sets of experiments one of which is regarding the behavior of action elimination algorithm, UCB and LUCB while the other is relevant to stopping times of the algorithms in literatures.\n",
    "The result of the first group of experiment presents that  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(b) Reproduce the results in Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import heapq\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import add\n",
    "\n",
    "\n",
    "class UCBBandit:\n",
    "    \"\"\"\n",
    "    UCBBandit\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, arms, delta, eps,total_steps):\n",
    "        self.q_true = list(reversed(np.linspace(0, 1, arms)))\n",
    "        self.arms = arms\n",
    "        self.eps = eps\n",
    "        self.delta = delta\n",
    "        self.est_dstr = []\n",
    "        self.total_steps=total_steps\n",
    "        self.H1=0\n",
    "        for idx in range(1,arms):\n",
    "            self.H1 = (1/(self.q_true[0]-self.q_true[idx])**2)+ self.H1\n",
    "        for i in range(arms):\n",
    "            self.est_dstr.append([])\n",
    "\n",
    "\n",
    "\n",
    "    def _calculate_c(self):\n",
    "        \"\"\"\n",
    "        Calculate the C_(i,t) = 2U(T_i(t), delta/n)\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        for arm, _ in enumerate(self.q_est):\n",
    "            exp_1 = 1 + self.eps ** 0.5\n",
    "            exp_2 = 1 + self.eps\n",
    "            exp_3 = (1 + self.eps) * self.N[arm]\n",
    "            exp = exp_1 * \\\n",
    "                  np.sqrt((exp_2 * np.log((np.log(exp_3 + 2)) / self.delta)) / (self.N[arm]))\n",
    "            res.append(exp)\n",
    "        return res\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Initializes the UCBBandit with default values before each trial\n",
    "        \"\"\"\n",
    "        self.q_est = np.zeros(self.arms)\n",
    "        self.N = np.zeros(self.arms)\n",
    "        self.history = []\n",
    "        self.time = 0\n",
    "        for arm, _ in enumerate(self.q_est):\n",
    "            self.play_arm(arm)\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\"\n",
    "        Select the arm to play for the Bandit\n",
    "        \"\"\"\n",
    "        h = np.argmax(self.q_est)\n",
    "        C = self._calculate_c()\n",
    "        QC = np.sum([self.q_est, C], axis=0)\n",
    "        l_ = heapq.nlargest(2, range(self.arms), key=QC.__getitem__)\n",
    "        l = l_[0] if h != l_[0] else l_[1]\n",
    "        # Select arm\n",
    "        arm = l_[0]\n",
    "        # Check stopping criterion\n",
    "        if self.q_est[h] - C[h] > self.q_est[l] + C[l]:\n",
    "            self.extend_history(h)\n",
    "            return arm, True\n",
    "        return arm, False\n",
    "\n",
    "    def play_arm(self, arm):\n",
    "        \"\"\"\n",
    "        Play the arm and update its value estimate\n",
    "        \"\"\"\n",
    "        self.time += 1.0\n",
    "        self.N[arm] += 1.0\n",
    "\n",
    "        reward = np.random.normal(self.q_true[arm], 0.5)\n",
    "        self.sample_history(arm)\n",
    "        self.q_est[arm] += 1.0 / self.N[arm] * (reward - self.q_est[arm])\n",
    "\n",
    "    def sample_history(self, arm):\n",
    "        \"\"\"\n",
    "        Record the history of sampling\n",
    "        \"\"\"\n",
    "        self.history.append(arm)\n",
    "\n",
    "    def extend_history(self,output_arm):\n",
    "        \"\"\"\n",
    "        When the algorithm terminates before it hits total time steps\n",
    "        \"\"\"\n",
    "        while len(self.history)<self.total_steps:\n",
    "            self.history.append(output_arm)\n",
    "\n",
    "    def get_estimate_probability(self):\n",
    "        \"\"\"\n",
    "        get accumulated Pr{I_t=i} over trials\n",
    "        \"\"\"\n",
    "        for t in range(self.arms,self.total_steps):\n",
    "            left_index=max(0,t-self.arms+1)\n",
    "            right_index=t\n",
    "            for arm in range(self.arms):\n",
    "                prob=self.history[left_index:right_index+1].count(arm)/(right_index-left_index+1)\n",
    "                if t >= len(self.est_dstr[arm]):\n",
    "                    self.est_dstr[arm].append(prob)\n",
    "                else:\n",
    "                    self.est_dstr[arm][t] += prob\n",
    "\n",
    "\n",
    "    def average_est_dstr(self,trials):\n",
    "        \"\"\"\n",
    "         average the est_dstr\n",
    "         \"\"\"\n",
    "        for arm in range(self.arms):\n",
    "            self.est_dstr[arm]=list(map((1/trials).__mul__, self.est_dstr[arm]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AEBandit(UCBBandit):\n",
    "    def __init__(self, arms, delta, eps,total_steps,r_k):\n",
    "        self.q_true = list(reversed(np.linspace(0, 1, arms)))\n",
    "        self.arms = arms\n",
    "        self.eps = eps\n",
    "        self.delta = delta\n",
    "        self.est_dstr = []\n",
    "        self.total_steps=total_steps\n",
    "        for i in range(arms):\n",
    "            self.est_dstr.append([])\n",
    "        self.r_k = r_k\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Initializes the UCBBandit with default values before each trial\n",
    "        \"\"\"\n",
    "        self.q_est = np.zeros(self.arms)\n",
    "        self.N = np.zeros(self.arms)\n",
    "        self.history = []\n",
    "        self.time = 0\n",
    "        self.Omega = set()\n",
    "        for arm in range(self.arms):\n",
    "            self.Omega.add(str(arm))\n",
    "\n",
    "    def reference_arm(self):\n",
    "        return np.argmax(self.q_est)\n",
    "\n",
    "    def _calculate_c(self,arm):\n",
    "        exp_1 = 1 + self.eps ** 0.5\n",
    "        exp_2 = 1+self.eps\n",
    "        exp_3 = (1 + self.eps) * self.N[arm]\n",
    "        exp = exp_1 * \\\n",
    "        np.sqrt((exp_2 * np.log((np.log(exp_3+2)) / (self.delta/self.N[arm]) )) / ( self.N[arm]))\n",
    "        return exp\n",
    "\n",
    "    def update_omega(self,reference_arm):\n",
    "        removed_arms=set()\n",
    "        for arm in self.Omega:\n",
    "            C_a=self._calculate_c(reference_arm)\n",
    "            C_i=self._calculate_c(int(arm))\n",
    "            if self.q_est[reference_arm]-C_a >= self.q_est[int(arm)]+C_i:\n",
    "                removed_arms.add(arm)\n",
    "        self.Omega = self.Omega-removed_arms\n",
    "\n",
    "    def learning(self):\n",
    "        for i in range(self.total_steps):\n",
    "            for arm in self.Omega:\n",
    "                self.play_arm(int(arm))\n",
    "            reference_arm = self.reference_arm()\n",
    "            self.update_omega(reference_arm)\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "class LUCBBandit(UCBBandit):\n",
    "    def select_arm(self):\n",
    "        \"\"\"\n",
    "        Select the arm to play for the Bandit\n",
    "        \"\"\"\n",
    "        h = np.argmax(self.q_est)\n",
    "        C = self._calculate_c()\n",
    "        QC = np.sum([self.q_est, C], axis=0)\n",
    "        l_ = heapq.nlargest(2, range(self.arms), key=QC.__getitem__)\n",
    "        l = l_[0] if h != l_[0] else l_[1]\n",
    "        # Select arm\n",
    "        # Check stopping criterion\n",
    "        if self.q_est[h] - C[h] > self.q_est[l] + C[l]:\n",
    "            self.extend_history(h)\n",
    "            return h,l, True\n",
    "        return h,l,False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action Elimination Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--delta DELTA] [--epsilon EPSILON]\n",
      "                             [--trials TRIALS] [--steps STEPS] [--r_k R_K]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Administrator\\AppData\\Roaming\\jupyter\\runtime\\kernel-f18e4f47-ce39-493f-a4af-a0c5a9e783b9.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\python\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--delta', type=float, default=0.1)\n",
    "    parser.add_argument('--epsilon', type=float, default=0.01)\n",
    "    parser.add_argument('--trials', type=int, default=10)\n",
    "    parser.add_argument('--steps', type=int, default=2700)\n",
    "    parser.add_argument('--r_k', type=int, default=1)\n",
    "    args = parser.parse_args()\n",
    "    bandit_AE = AEBandit(arms=6, delta=args.delta, eps=args.epsilon,total_steps=args.steps,r_k=args.r_k)\n",
    "    for trial in range(args.trials):\n",
    "        bandit_AE.initialize()\n",
    "        bandit_AE.learning()\n",
    "        bandit_AE.get_estimate_probability()\n",
    "    bandit_AE.average_est_dstr(args.trials)\n",
    "    return bandit\n",
    "\n",
    "\n",
    "b=main()\n",
    "color_set=['b','g','r','c','y','k']\n",
    "\n",
    "line0,= plt.plot(range(len(b.est_dstr[0]))/b.H1,b.est_dstr[0],'--',color = 'b',markersize=2,label=\"$\\mu_0=1.0$\")\n",
    "line1,= plt.plot(range(len(b.est_dstr[1]))/b.H1,b.est_dstr[1],'--',color = 'g',markersize=2,label=\"$\\mu_1=0.8$\")\n",
    "line2,= plt.plot(range(len(b.est_dstr[2]))/b.H1,b.est_dstr[2],'--',color = 'r',markersize=2,label=\"$\\mu_2=0.6$\")\n",
    "line3,= plt.plot(range(len(b.est_dstr[3]))/b.H1,b.est_dstr[3],'--',color = 'c',markersize=2,label=\"$\\mu_3=0.4$\")\n",
    "line4,= plt.plot(range(len(b.est_dstr[4]))/b.H1,b.est_dstr[4],'--',color = 'y',markersize=2,label=\"$\\mu_4=0.2$\")\n",
    "line5,= plt.plot(range(len(b.est_dstr[5]))/b.H1,b.est_dstr[5],'--',color = 'k',markersize=2,label=\"$\\mu_5=0.0$\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Number of pulls (units of H1)\")\n",
    "plt.ylabel(\"$P(I_t=i)$\")\n",
    "plt.yticks(np.arange(0,1.1,0.1))\n",
    "plt.legend(bbox_to_anchor=(0, 0.5, 0.4, 0.5))\n",
    "plt.show()\n",
    "print(b.history)\n",
    "#if __name__ == '__main__':\n",
    "#    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
